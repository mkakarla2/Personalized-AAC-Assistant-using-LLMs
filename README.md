# Personalized AAC Assistant using RAG and LLaMA 3

**Authors**: Mohan Kakarla, Udit Brahmadevara    
**Domain**: Human-Centered AI | Natural Language Processing | Retrieval Augmented Generation | Assistive Communication | Personalization  

---

## ðŸ§  Abstract

This project explores an **Augmentative and Alternative Communication (AAC)** system powered by a **Retrieval-Augmented Generation (RAG)** pipeline and a **fine-tuned LLaMA 3 8B-Instruct** model. It is designed to support individuals with limited verbal communication by suggesting **personalized, context-aware responses** that reflect their unique persona and communication style.



## ðŸŽ¯ Project Objectives

- ðŸ§  Aid non-speaking individuals by generating meaningful verbal suggestions based on intent and personal communication profiles.
- ðŸ”Ž Use **RAG** to retrieve relevant past narratives or experiences to ground model outputs.
- ðŸ¤– Employ a **fine-tuned LLaMA 3 8B-Instruct** LLM to ensure fluent, coherent, and identity-preserving communication.
- ðŸ§© Build an accessible, modular, and visually intuitive AAC interface using **Streamlit**.

---

## ðŸ§° Core Technologies

| Component             | Description                                                               |
|-----------------------|---------------------------------------------------------------------------|
| ðŸ§  LLM Backbone        | Fine-tuned LLaMA 3 8B-Instruct model (hosted externally on Kaggle)        |
| ðŸ§© RAG Framework       | FAISS-based similarity search with Sentence-BERT embeddings               |
| ðŸ“„ User Profiles       | JSON-based profiles storing communication style, history, tone, summary  |
| ðŸ’¬ Frontend            | Streamlit chat UI with multiple responses and user-influence inputs       |
| ðŸ—ƒï¸ Text Chunking       | RecursiveCharacterTextSplitter with dynamic chunk size and overlap        |

---

## ðŸ” Key Features

- ðŸ‘¥ **Multiple user personas**: Select between different tones (empathetic, humorous, concise, etc.).
- ðŸ§¾ **Customizable prompts**: Add "influence" like *Be humorous*, *Ask a question*, *Agree*.
- ðŸ” **Multiple LLM responses**: Generate and choose from multiple options for each input.
- âš™ï¸ **Chunked RAG retrieval**: Customize chunking for better retrieval of personal context.
- ðŸ› ï¸ **Debugging panel**: Inspect prompt formatting and retrieved narratives live.

---
## ðŸ—ï¸ System Architecture
![System Architecture](media/architecture.png) 
- **Frontend**: Streamlit interface
   ![UI Screenshot](media/ui.png)
- **Backend**:  
  - Finetuned LLaMA model (loaded via HuggingFace or local path)  
  - FAISS-based vector store to store and retrieve user-specific memory  
  - Personalization layer (prompt engineering with profiles)  
- **Data Format**: JSON file for user histories Refer to [`user_profiles.json`](user_profiles.json) for:  

   
  

---
## ðŸ“„ Academic Report

Refer to [`report.pdf`](report.pdf) for:

- Background on AAC systems and user-centered design.
- Model fine-tuning pipeline, hyperparameters, and evaluation.
- Dataset preparation and tokenizer configuration.
- System usability insights, limitations, and future work.

---

## ðŸ—‚ï¸ File Structure

```
â”œâ”€â”€ app.py                  # Streamlit web app (production entrypoint)
â”œâ”€â”€ Code.ipynb              # Core development notebook
â”œâ”€â”€ Training.ipynb          # Fine-tuning LLaMA 3 notebook
â”œâ”€â”€ user_profiles.json      # Communication style + memory settings
â”œâ”€â”€ report.pdf              # Academic report
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # Project overview
â”œâ”€â”€ Demo.mp4                # Walkthrough video
â””â”€â”€ media/
    â”œâ”€â”€ architecture.png    # System architecture diagram
    â””â”€â”€ ui.png              # UI screenshot
```

---

## ðŸ’¾ Finetuned Model

ðŸš¨ Due to GitHub storage limits, the fine-tuned LLaMA 3 8B-Instruct model is hosted externally:

ðŸ“¥ [Download Finetuned Model (~5GB)](https://www.kaggle.com/datasets/mohankumarkakarla/finetuned/data)

> After downloading, place the model inside:

```
./models/llm_finetuned/
```

Or for **Kaggle/Streamlit Cloud**, set `MERGED_MODEL_PATH` in `app.py` to the model directory location.

---

## ðŸš€ Running the App

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Run the Streamlit frontend

```bash
streamlit run app.py
```

> âœ… **Note**: If running inside a notebook (e.g., Kaggle), the last cell in `Code.ipynb` should launch:

```python
!streamlit run /kaggle/input/stream/fast_app.py
```

---

## ðŸ“œ License

This project is open-sourced for **academic and research purposes only**. Please refer to [`LICENSE`](LICENSE) for more information.

---
