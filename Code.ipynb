{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11667281,"sourceType":"datasetVersion","datasetId":7322350},{"sourceId":11763807,"sourceType":"datasetVersion","datasetId":7385223},{"sourceId":11763849,"sourceType":"datasetVersion","datasetId":7385248}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install streamlit pyngrok faiss-cpu sentence-transformers \n!pip install \"bitsandbytes>=0.43.1\" --quiet\n!pip install sentence-transformers bert-score scikit-learn pandas nltk --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import faiss\nimport pickle\nimport os\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport traceback\nimport json\nimport os\nimport subprocess\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nfrom bert_score import score as bert_score\nimport nltk\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T00:09:29.983873Z","iopub.execute_input":"2025-05-11T00:09:29.984283Z","iopub.status.idle":"2025-05-11T00:09:29.989562Z","shell.execute_reply.started":"2025-05-11T00:09:29.984252Z","shell.execute_reply":"2025-05-11T00:09:29.989007Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def load_user_profiles(path: str = \"user_profiles.json\"):\n    if not os.path.exists(path):\n        st.error(f\"Could not find user profiles file at {path}\")\n        return {}\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return data\n\nUSER_PROFILES = load_user_profiles(\"/kaggle/input/user-profiles/user_profiles.json\")\n\noutput_rag_dir = \"/kaggle/working/rag_indices\"\nembedding_model_name = \"all-MiniLM-L6-v2\"\nos.makedirs(output_rag_dir, exist_ok=True) \n\ntry:\n    print(f\"Loading embedding model: {embedding_model_name}\")\n    embedding_model = SentenceTransformer(embedding_model_name)\n    embedding_dim = embedding_model.get_sentence_embedding_dimension()\n    print(f\"Embedding model loaded. Dimension: {embedding_dim}\")\n\n    for profile_key, profile_data in USER_PROFILES.items():\n        print(f\"\\nProcessing profile: {profile_key}\")\n        docs_to_index = profile_data.get(\"personal_docs\", [])\n\n        if not docs_to_index:\n            print(f\"  No personal_docs found for {profile_key}. Skipping index creation.\")\n            continue\n\n        faiss_index_path = os.path.join(output_rag_dir, f\"index_{profile_key}.faiss\")\n        narratives_path = os.path.join(output_rag_dir, f\"docs_{profile_key}.pkl\")\n\n        print(f\"  Generating {len(docs_to_index)} embeddings for {profile_key}\")\n        narrative_embeddings = embedding_model.encode(docs_to_index, convert_to_numpy=True, show_progress_bar=False) # Less verbose\n        if narrative_embeddings.dtype != np.float32:\n            narrative_embeddings = narrative_embeddings.astype(np.float32)\n        print(f\"  Embeddings generated.\")\n\n        print(f\"  Building FAISS index for {profile_key}\")\n        index = faiss.IndexFlatL2(embedding_dim)\n        index.add(narrative_embeddings)\n        print(f\"  Saving FAISS index ({index.ntotal} vectors) to: {faiss_index_path}\")\n        faiss.write_index(index, faiss_index_path)\n\n        print(f\"  Saving narratives list to: {narratives_path}\")\n        with open(narratives_path, 'wb') as f: pickle.dump(docs_to_index, f)\n\n        print(f\"  Profile {profile_key} RAG components saved.\")\n\nexcept Exception as e:\n    print(f\"ERROR during RAG setup: {e}\")\n    traceback.print_exc()\n\nprint(\"\\n Per-Persona RAG Index Creation Complete \")","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:12:13.106312Z","iopub.execute_input":"2025-05-10T23:12:13.106536Z","iopub.status.idle":"2025-05-10T23:12:14.186262Z","shell.execute_reply.started":"2025-05-10T23:12:13.106518Z","shell.execute_reply":"2025-05-10T23:12:14.185511Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading embedding model: all-MiniLM-L6-v2\nEmbedding model loaded. Dimension: 384\n\nProcessing profile: Default\n  No personal_docs found for Default. Skipping index creation.\n\nProcessing profile: Alex\n  Generating 10 embeddings for Alex\n  Embeddings generated.\n  Building FAISS index for Alex\n  Saving FAISS index (10 vectors) to: /kaggle/working/rag_indices/index_Alex.faiss\n  Saving narratives list to: /kaggle/working/rag_indices/docs_Alex.pkl\n  Profile Alex RAG components saved.\n\nProcessing profile: Sam\n  Generating 8 embeddings for Sam\n  Embeddings generated.\n  Building FAISS index for Sam\n  Saving FAISS index (8 vectors) to: /kaggle/working/rag_indices/index_Sam.faiss\n  Saving narratives list to: /kaggle/working/rag_indices/docs_Sam.pkl\n  Profile Sam RAG components saved.\n\nProcessing profile: AlexCarter\n  Generating 24 embeddings for AlexCarter\n  Embeddings generated.\n  Building FAISS index for AlexCarter\n  Saving FAISS index (24 vectors) to: /kaggle/working/rag_indices/index_AlexCarter.faiss\n  Saving narratives list to: /kaggle/working/rag_indices/docs_AlexCarter.pkl\n  Profile AlexCarter RAG components saved.\n\nProcessing profile: NinaMarchand\n  Generating 19 embeddings for NinaMarchand\n  Embeddings generated.\n  Building FAISS index for NinaMarchand\n  Saving FAISS index (19 vectors) to: /kaggle/working/rag_indices/index_NinaMarchand.faiss\n  Saving narratives list to: /kaggle/working/rag_indices/docs_NinaMarchand.pkl\n  Profile NinaMarchand RAG components saved.\n\nProcessing profile: SarahDunphy\n  Generating 22 embeddings for SarahDunphy\n  Embeddings generated.\n  Building FAISS index for SarahDunphy\n  Saving FAISS index (22 vectors) to: /kaggle/working/rag_indices/index_SarahDunphy.faiss\n  Saving narratives list to: /kaggle/working/rag_indices/docs_SarahDunphy.pkl\n  Profile SarahDunphy RAG components saved.\n\n Per-Persona RAG Index Creation Complete \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"nltk.download(\"punkt\")\n\n# Load embedding model\nembed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Sample dataset\ndf = pd.DataFrame([\n    {\n        \"prompt\": \"Do you have any pets?\",\n        \"reference\": \"I have a pet golden retriever named Max.\",\n        \"generated_response\": \"Yes! Max, my golden retriever, is my loyal sidekick and best friend.\"\n    },\n    {\n        \"prompt\": \"What kind of movies do you enjoy watching?\",\n        \"reference\": \"I love sci-fi movies set in space with warp speed and cool tech. Star Trek is my favorite!\",\n        \"generated_response\": \"I love watching Sci-fi preferably set in space, with a dash of warp speed and a pinch of futuristic tech! (Wink) Star Trek Movies\"\n    }\n])\n\nresults = []\n\nfor _, row in df.iterrows():\n    prompt = row[\"prompt\"]\n    reference = row[\"reference\"]\n    generated = row[\"generated_response\"]\n\n    # Cosine similarity\n    ref_emb = embed_model.encode([reference], convert_to_numpy=True)\n    gen_emb = embed_model.encode([generated], convert_to_numpy=True)\n    cos_sim = cosine_similarity(ref_emb, gen_emb)[0][0]\n\n    # BERTScore\n    P, R, F1 = bert_score([generated], [reference], lang=\"en\", rescale_with_baseline=True)\n\n    results.append({\n        \"prompt\": prompt,\n        \"reference\": reference,\n        \"generated_response\": generated,\n        \"cosine_similarity\": round(cos_sim, 4),\n        \"bertscore_precision\": round(P[0].item(), 4),\n        \"bertscore_recall\": round(R[0].item(), 4),\n        \"bertscore_f1\": round(F1[0].item(), 4)\n    })\n\n# Save results\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"rag_model_scores_no_bleu.csv\", index=False)\nresults_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T00:00:23.444069Z","iopub.execute_input":"2025-05-11T00:00:23.444829Z","iopub.status.idle":"2025-05-11T00:00:27.452038Z","shell.execute_reply.started":"2025-05-11T00:00:23.444795Z","shell.execute_reply":"2025-05-11T00:00:27.451368Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87391e98c544af9b98cb356d0b92901"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9080844f1c194bd8ac5fa2cd85fb2000"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8be18b46cea048a1bad4c67d8e585ccd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b35a0f02f746d9a5b4f600828481ea"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                       prompt  \\\n0                       Do you have any pets?   \n1  What kind of movies do you enjoy watching?   \n\n                                           reference  \\\n0           I have a pet golden retriever named Max.   \n1  I love sci-fi movies set in space with warp sp...   \n\n                                  generated_response  cosine_similarity  \\\n0  Yes! Max, my golden retriever, is my loyal sid...             0.6962   \n1  I love watching Sci-fi preferably set in space...             0.9202   \n\n   bertscore_precision  bertscore_recall  bertscore_f1  \n0               0.4074            0.5213        0.4646  \n1               0.4811            0.6034        0.5423  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>reference</th>\n      <th>generated_response</th>\n      <th>cosine_similarity</th>\n      <th>bertscore_precision</th>\n      <th>bertscore_recall</th>\n      <th>bertscore_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you have any pets?</td>\n      <td>I have a pet golden retriever named Max.</td>\n      <td>Yes! Max, my golden retriever, is my loyal sid...</td>\n      <td>0.6962</td>\n      <td>0.4074</td>\n      <td>0.5213</td>\n      <td>0.4646</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What kind of movies do you enjoy watching?</td>\n      <td>I love sci-fi movies set in space with warp sp...</td>\n      <td>I love watching Sci-fi preferably set in space...</td>\n      <td>0.9202</td>\n      <td>0.4811</td>\n      <td>0.6034</td>\n      <td>0.5423</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"#Installing ngrok\n!wget -q -c -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n!unzip -qq -n ngrok-v3-stable-linux-amd64.zip\n\n!./ngrok config add-authtoken 2wVBgrUhAnFME3W6pdW6D7C7eYO_4ezkVq2JnLrVHSryLTXPY","metadata":{"execution":{"iopub.status.busy":"2025-05-11T00:01:34.750295Z","iopub.execute_input":"2025-05-11T00:01:34.750637Z","iopub.status.idle":"2025-05-11T00:01:36.605122Z","shell.execute_reply.started":"2025-05-11T00:01:34.750613Z","shell.execute_reply":"2025-05-11T00:01:36.604190Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"try:\n  subprocess.run([\"killall\", \"ngrok\"], check=True, capture_output=True)\n  print(\"Existing ngrok processes killed.\")\nexcept (subprocess.CalledProcessError, FileNotFoundError):\n  print(\"No existing ngrok process found or killall not available.\") \n\nprint(\"Starting ngrok tunnel\")\nnohup_cmd = \"nohup ./ngrok http 8501 > ngrok.log 2>&1 &\"\nos.system(nohup_cmd)\n\nimport time\ntime.sleep(5) \ntry:\n  import requests\n  time.sleep(2) \n  localhost_url = \"http://localhost:4040/api/tunnels\" # Ngrok's local API\n  tunnel_info = requests.get(localhost_url).json()\n  # Find the https tunnel URL\n  public_url = None\n  for tunnel in tunnel_info.get(\"tunnels\", []):\n      if tunnel.get(\"proto\") == \"https\":\n          public_url = tunnel.get(\"public_url\")\n          break \n\n  if public_url:\n      print(f\"Ngrok tunnel active. Public URL: {public_url}\")\n      from IPython.display import display, HTML\n      display(HTML(f'<a href=\"{public_url}\" target=\"_blank\">{public_url}</a>'))\n  else:\n      print(\"Could not automatically fetch ngrok URL. Check ngrok.log or manually check http://localhost:4040 if API is running.\")\n      print(\"Sometimes ngrok takes longer to start. Try checking the log file:\")\n      !cat ngrok.log \n\nexcept Exception as e:\n  print(f\"Could not connect to ngrok API to get URL automatically: {e}\")\n  print(\"Check ngrok status and log file ('ngrok.log'). The tunnel might still be active.\")\n  !cat ngrok.log \n\nprint(\"Ngrok setup complete running in background.\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T00:01:41.965159Z","iopub.execute_input":"2025-05-11T00:01:41.965989Z","iopub.status.idle":"2025-05-11T00:01:48.996898Z","shell.execute_reply.started":"2025-05-11T00:01:41.965958Z","shell.execute_reply":"2025-05-11T00:01:48.996004Z"},"trusted":true},"outputs":[{"name":"stdout","text":"No existing ngrok process found or killall not available.\nStarting ngrok tunnel\nNgrok tunnel active. Public URL: https://79dd-34-171-229-14.ngrok-free.app\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<a href=\"https://79dd-34-171-229-14.ngrok-free.app\" target=\"_blank\">https://79dd-34-171-229-14.ngrok-free.app</a>"},"metadata":{}},{"name":"stdout","text":"Ngrok setup complete running in background.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!streamlit run /kaggle/input/stream/fast_app.py      ","metadata":{"execution":{"iopub.status.busy":"2025-05-11T00:01:48.998486Z","iopub.execute_input":"2025-05-11T00:01:48.998709Z","iopub.status.idle":"2025-05-11T00:07:45.183544Z","shell.execute_reply.started":"2025-05-11T00:01:48.998693Z","shell.execute_reply":"2025-05-11T00:07:45.182519Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.171.229.14:8501\u001b[0m\n\u001b[0m\n2025-05-11 00:02:01.805179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746921721.833072     213 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746921721.841367     213 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nLoading models from path: /kaggle/input/finetuned/\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:47<00:00, 23.50s/it]\nLLM and tokenizer loaded.\nEmbedding model loaded.\nAll models loaded.\n2025-05-11 00:02:59.804 Examining the path of torch.classes raised:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n    if asyncio.get_running_loop().is_running():\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: no running event loop\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n    potential_paths = extract_paths(module)\n                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n    lambda m: list(m.__path__._path),\n                   ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\nBuilding RAG index for Alex with settings key: rag_06824f52f8d6197f2129f691f504ee1b\nEmbedding 10 chunks for rag_06824f52f8d6197f2129f691f504ee1b...\nEmbedding complete.\nFAISS index built (10 vectors) for rag_06824f52f8d6197f2129f691f504ee1b.\nBatches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 99.02it/s]\nRetrieved 1 docs for query 'Do you have any pets?\t' (Alex).\n\n--- Generated Prompt ---\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are Alex. Alex is friendly, enjoys sci-fi (Star Trek), loves his dog Max and sister Chloe in London. Communication guideline: slightly informal, friendly, uses humor.\n\nRelevant context:\n- I have a pet golden retriever named Max.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nDo you have any pets?\t\n(When responding, please consider this direction: Concise response, )<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nDo you have any pets?\t<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n----------------------\n\nGenerating 2 responses...\nUsing device: cuda:0\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nLLM gen took 55.94s.\n^C\n\u001b[34m  Stopping...\u001b[0m\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}